{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb94b39-a418-47e2-b670-bb519a36cf28",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "%pip install semantic-link-labs --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661e6130-9d19-4d99-b3b3-2c0cff3b5262",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-22T22:57:45.4864701Z",
       "execution_start_time": "2025-08-22T22:57:40.8367189Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "ee3ae721-906a-4a77-b5af-039a50533510",
       "queued_time": "2025-08-22T22:55:45.4703464Z",
       "session_id": "cf52a2e4-21bf-463e-8d0e-c594a0a2b084",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, cf52a2e4-21bf-463e-8d0e-c594a0a2b084, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sempy_labs as labs\n",
    "import sempy_labs.admin as labs_admin\n",
    "from pyspark.sql import Row\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b47641-5582-4581-bafa-4023ff853bfd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-22T22:57:45.7591723Z",
       "execution_start_time": "2025-08-22T22:57:45.4887844Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8c0383b1-3ffb-4b04-8874-7f86c8732cdf",
       "queued_time": "2025-08-22T22:55:45.9538296Z",
       "session_id": "cf52a2e4-21bf-463e-8d0e-c594a0a2b084",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, cf52a2e4-21bf-463e-8d0e-c594a0a2b084, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_to_table(level, message):\n",
    "    now = datetime.datetime.utcnow().isoformat()\n",
    "    log_df = spark.createDataFrame([Row(timestamp=now, level=level, message=message)])\n",
    "    log_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"semanticModel_conversion_toLarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d7be8-8364-485f-88ee-ea28ad746b84",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "current_user = mssparkutils.env.getUserName()\n",
    "workspaces_df = spark.sql(f\"Select DISTINCT Dataset_Id ,Dataset_Name, Workspace_Id from models_discovery_status where Status = 'Success'\")\n",
    "for row in workspaces_df.collect():\n",
    "    ws_id = row['Workspace_Id']\n",
    "    ds_id = row['Dataset_Id']\n",
    "    ds_name = row['Dataset_Name']\n",
    "    try:\n",
    "      labs.set_semantic_model_storage_format(dataset=ds_id, storage_format='Large', workspace=ws_id)\n",
    "      print(f\" -Format converted successfully.'{ds_name}'.\")\n",
    "      log_to_table(\"INFO\", f\"Format converted successfully. {ds_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå FAILED: An error occurred while processing model {ds_name}.\")\n",
    "        print(f\"    Error details: {e}\")\n",
    "        log_to_table(\"ERROR\", f\"Exception occurred: in dataset {ds_name} error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {},
   "lakehouse": {
    "default_lakehouse": "82b79bac-7938-43e2-9292-ca031bde5947",
    "default_lakehouse_name": "metadatascan_global_lh_0822",
    "default_lakehouse_workspace_id": "b1317605-ef25-462c-9dda-82a68b53f242",
    "known_lakehouses": [
     {
      "id": "82b79bac-7938-43e2-9292-ca031bde5947"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
