{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303ef1a3-3cb2-47ed-a89a-7ff6baa4262a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-23T17:41:40.4835097Z",
       "execution_start_time": "2025-08-23T17:41:18.4345243Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "23a860c6-29c4-42b8-9c00-295a6a31e724",
       "queued_time": "2025-08-23T17:41:05.7737836Z",
       "session_id": "8af66299-d363-4ce8-b51f-ea5975812057",
       "session_start_time": "2025-08-23T17:41:05.7740944Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        3,
        4,
        5,
        6,
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 8af66299-d363-4ce8-b51f-ea5975812057, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Warning: PySpark kernel has been restarted to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install semantic-link-labs --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b127ae97-5bb6-4078-86a9-671ba315abab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-23T17:41:50.0058317Z",
       "execution_start_time": "2025-08-23T17:41:46.599037Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c706e71e-ca2e-40f7-9171-15cb49e1a1cd",
       "queued_time": "2025-08-23T17:41:05.9075911Z",
       "session_id": "8af66299-d363-4ce8-b51f-ea5975812057",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 8af66299-d363-4ce8-b51f-ea5975812057, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sempy_labs as labs\n",
    "import sempy_labs.admin as labs_admin\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398fcc7-0657-4841-80c3-b736bf41f785",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-23T17:41:50.2781345Z",
       "execution_start_time": "2025-08-23T17:41:50.0079432Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1709d671-b404-49c2-8f5d-139603612944",
       "queued_time": "2025-08-23T17:41:06.2556627Z",
       "session_id": "8af66299-d363-4ce8-b51f-ea5975812057",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 8af66299-d363-4ce8-b51f-ea5975812057, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_to_table(level, workspaceid, message):\n",
    "    now = datetime.datetime.utcnow().isoformat()\n",
    "    log_df = spark.createDataFrame([Row(timestamp=now, level=level, workspaceId=workspaceid, message=message)])\n",
    "    log_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"largeToSmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc7b297-4f8b-48d7-b916-0f4923b96516",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-23T17:41:50.5449239Z",
       "execution_start_time": "2025-08-23T17:41:50.2802397Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f35b7690-58c1-469a-9530-6179d49b6d9e",
       "queued_time": "2025-08-23T17:41:06.5240179Z",
       "session_id": "8af66299-d363-4ce8-b51f-ea5975812057",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 8af66299-d363-4ce8-b51f-ea5975812057, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backup_model(ds_name, workspace_id):\n",
    "    try:\n",
    "        labs.set_semantic_model_storage_format(dataset=ds_name, storage_format='Small', workspace=workspace_id)\n",
    "        \n",
    "        msg = f\"Informational: format conversion to dataset for : {ds_name} in workspace {workspace_id}.\"\n",
    "        log_to_table(\"INFO\",workspace_id,msg)\n",
    "        return msg\n",
    "    except Exception as e:\n",
    "       msg = f\"format conversion to dataset for: {ds_name} in workspace {workspace_id} with error: {e}\"\n",
    "       log_to_table(\"ERROR\",workspace_id,msg)\n",
    "       return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fefb3a-a911-4c58-b0dc-028753c85d72",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "workspaces_df = spark.sql(f\"Select DISTINCT Dataset_Id ,Dataset_Name, Workspace_Id from models_discovery_status where Status <> 'Success' and Size > 0\")\n",
    "try:\n",
    "      msg = f\"format conversion Started.\"\n",
    "      log_to_table(\"INFO\",\"N/A\",msg)\n",
    "\n",
    "      workspace_list = [(row['Dataset_Id'], row['Workspace_Id']) for row in workspaces_df.collect()]\n",
    "      \n",
    "      #print(f\"Datasets to process: {workspace_list}\")\n",
    "      print(f\"Informational: format conversion Started.\")\n",
    "\n",
    "      with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "         futures = [executor.submit(backup_model, ds,ws) for ds,ws in workspace_list]\n",
    "         for future in as_completed(futures):\n",
    "            try: \n",
    "               print(future.result())  # This will raise any exceptions caught during execution\n",
    "            except Exception as e:\n",
    "               print(f\"Exception in future: {e}\")\n",
    "\n",
    "      msg = f\"format conversion Operation Completed.\"\n",
    "      log_to_table(\"INFO\",\"N/A\",msg)\n",
    "      print(f\"Informational: format conversion Completed.\")\n",
    "except Exception as e:\n",
    "      print(f\"Informational: conversion Failed\")\n",
    "      msg = f\"ðŸ”´Error in converison operation: {e}\"\n",
    "      log_to_table(\"ERROR\",\"N/A\",msg)                "
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "81d02931-4a16-4e13-9e03-aefe5b47bc7f",
    "default_lakehouse_name": "metadatascan_pfna_lh_0823",
    "default_lakehouse_workspace_id": "b1317605-ef25-462c-9dda-82a68b53f242",
    "known_lakehouses": [
     {
      "id": "81d02931-4a16-4e13-9e03-aefe5b47bc7f"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
