{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4d815b-1d3e-4c85-9ede-e7ce5a635431",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-30T14:28:37.2089518Z",
       "execution_start_time": "2025-07-30T14:28:36.8367975Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2a26b6ed-c720-431e-a37f-27268cfa0946",
       "queued_time": "2025-07-30T14:23:52.0017972Z",
       "session_id": "a417a8a5-dba1-4a02-9e81-ca1019c93274",
       "session_start_time": "2025-07-30T14:23:52.0028691Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, a417a8a5-dba1-4a02-9e81-ca1019c93274, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Large Model Discovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2377f-c8b5-4036-98d9-3dcfbd56cdc9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# CMF - Large Sementic Models discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85ff9b-667e-43c0-a0db-6026953a0ae1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Key Changes and Improvements\n",
    "\n",
    "This version of the notebook incorporates several enhancements over the original:\n",
    "\n",
    "* **Dynamic Bin Calculation:**\n",
    "    * Introduced `p_target_models_per_bin` and `p_max_bins` parameters.\n",
    "    * If `p_num_bins` is not explicitly set (>0), the notebook calculates the optimal number of parallel bins based on the target models per bin, respecting the maximum limit and the total model count.\n",
    "    * Defaults to sequential processing (1 bin) if no binning strategy is defined.\n",
    "\n",
    "* **Improved Robustness in Bin Handling for Parallelism::**\n",
    "    * Models are now assigned to bins using a method (`numpy.floor` division on a sequential index) that ensures a much more even distribution compared to the previous modulo approach, leading to better load balancing across parallel runs.\n",
    "    * When preparing for parallel execution with `notebookutils.notebook.runMultiple`, if the query to get distinct `Bin` numbers from the `models_discovery_status` table fails or returns an empty list (despite models being present), the script attempts to reconstruct the list of bins to iterate over (from 1 to `actual_num_bins`). This helps prevent the parallel processing step from failing unnecessarily.\n",
    "\n",
    "\n",
    "* **Robust Delta Table Handling:**\n",
    "    * Introduced an `_initialize_delta_table` function to create or validate the schema of the output Delta table (`models_discovery_status`) before writing, preventing schema mismatch errors on subsequent runs.\n",
    "    * Uses an explicit schema (`StructType`) for the Delta table for consistency.\n",
    "    * Initializes model `Size` to `-1` (LongType) to clearly differentiate pending/unknown status from an actual size of 0 bytes.\n",
    "    * Added a `Last Updated` timestamp column for better tracking.\n",
    "\n",
    "* **Optional Size Filtering:**\n",
    "    * Added the `p_min_model_size_gb` parameter.\n",
    "    * If set to a value greater than 0, models successfully scanned but found to be *smaller* than this threshold (in GB) will have their status marked as `Ignored (Size)`. Set to `0` or `None` to disable this filter.\n",
    "\n",
    "* **Enhanced Error Logging and Status Tracking:**\n",
    "    * The output Delta table now uses more descriptive statuses: `Pending`, `Success`, `Failure`, `Ignored (Default)`, `Ignored (Size)`.\n",
    "    * Detailed error messages are captured in the `Message` column.\n",
    "    * A **Final Summary Report** is generated by the main notebook instance *after* all processing completes. This report queries the Delta table to show counts per status, lists models that failed, and lists large models that were successfully processed.\n",
    "    * Improved exception handling for common issues like Fabric API errors (`FabricHTTPException`) and workspace access problems (`WorkspaceNotFoundException`).\n",
    "\n",
    "* **Sequential Fallback:**\n",
    "    * If the calculated number of bins is only one, the notebook automatically runs the processing logic sequentially within the main instance, avoiding the overhead of `notebookutils.runMultiple` for a single task.\n",
    "\n",
    "* **Improved Clarity and Structure:**\n",
    "    * Added more detailed print statements for better real-time progress monitoring.\n",
    "    * Code grouped into logical steps with explanatory comments.\n",
    "    * Necessary parameters are passed correctly to the sub-processes launched by `runMultiple`.\n",
    "\n",
    "* **Automated Workspace Access Management (Grant & Revoke):**\n",
    "    * **Granting Access (Step 0):** The script now includes a significant pre-processing step to grant the executing user 'Member' access to workspaces specified in the `p_only_workspaces` parameter. This uses `labs_admin.add_workspace_user`.\n",
    "        * It intelligently attempts to resolve workspace names to IDs using `fabric.resolve_workspace_id` for robustness before attempting the grant.\n",
    "        * It records which workspaces had permissions potentially altered by this step.\n",
    "    * **Revoking Access (Step 10 - Cleanup):** A crucial `finally` block ensures that the script attempts to revoke the 'Member' access it might have granted. This uses `labs_admin.delete_workspace_user` and the list of modified workspaces. This is a critical addition for maintaining a clean security posture.\n",
    "* **Specific Model Name Exclusion Filter:**\n",
    "    * A direct filter has been implemented within the sub-process/sequential processing loop to exclude any semantic model explicitly named 'Report Usage Metrics Model'.\n",
    "    * These excluded models are assigned a distinct status: `Ignored (Name Filter)`, which should be noted as a new possible status in the output Delta table.\n",
    "\n",
    "\n",
    "### How to Use\n",
    "\n",
    "1.  **Replace Code:** Substitute the code in your existing notebook with this improved version.\n",
    "2.  **Prerequisites:** Ensure all original prerequisites are met:\n",
    "    * A default Lakehouse is associated with the notebook's workspace.\n",
    "    * The `semantic-link-labs` library is installed in the Fabric Environment used by this notebook (install from PyPI).\n",
    "3.  **Configure Parameters:** Carefully review and adjust the parameters defined in the first cell (`--- Parameters ---`), paying close attention to:\n",
    "    * **Parallelism:** Choose *either* `p_target_models_per_bin` (recommended for dynamic sizing) *or* `p_num_bins` (for a fixed number of threads). Set the unused option to `0`. Remember `p_max_bins`.\n",
    "    * **Scoping:** Use `p_only_workspaces` or `p_exclude_workspaces` to limit which workspaces are scanned. Use workspace *names* or *GUIDs*.\n",
    "    * **Size Filtering:** Set `p_min_model_size_gb` to your desired minimum size (e.g., `10` for 10GB) or set it to `0` or `None` to process all non-default models regardless of size.\n",
    "    * **Backoff Strategy:** Adjust `p_backoff_threshold`, `p_backoff_delay`, and `p_backoff_delay_multiple` if you encounter API throttling issues (HTTP 429 errors).\n",
    "4.  **Run Notebook:** Execute the notebook cells sequentially.\n",
    "    * The primary instance (`_p_sub_bin == 0`) will:\n",
    "        * List models.\n",
    "        * Apply filters.\n",
    "        * Calculate bins and assign models.\n",
    "        * Save the initial list to the `models_discovery_status` Delta table in your default Lakehouse.\n",
    "        * Launch parallel notebook runs (if `num_bins > 1`) or run sequentially (if `num_bins == 1`).\n",
    "        * Wait for completion.\n",
    "        * Generate the Final Summary Report.\n",
    "    * Sub-process instances (`_p_sub_bin > 0`) will:\n",
    "        * Read their assigned models from the Delta table.\n",
    "        * Call `get_semantic_model_size` for each model.\n",
    "        * Apply size filtering if enabled.\n",
    "        * Update the status, size, and message for each model back into the Delta table.\n",
    "5.  **Review Results:** Check the output of the **Final Summary Report** cell and query the `models_discovery_status` Delta table in your Lakehouse for detailed results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63a607-a1c5-423d-b85e-6cf278cc7e46",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-30T14:29:01.1402253Z",
       "execution_start_time": "2025-07-30T14:28:37.2111118Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "02c629b2-90b2-4be4-8888-3283cf9394b4",
       "queued_time": "2025-07-30T14:23:52.0762085Z",
       "session_id": "a417a8a5-dba1-4a02-9e81-ca1019c93274",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, a417a8a5-dba1-4a02-9e81-ca1019c93274, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "# Processing Strategy\n",
    "# Option 1: Set p_num_bins directly (fixed number of parallel threads). Set p_target_models_per_bin = 0\n",
    "# Option 2: Set p_target_models_per_bin (desired models per thread). p_num_bins will be calculated.\n",
    "p_num_bins = 0                  # Override: Set > 0 to fix the number of bins, ignoring p_target_models_per_bin. Set to 1 for sequential processing.\n",
    "p_target_models_per_bin = 3    # Desired number of models per parallel bin (thread). Used if p_num_bins is 0 or less.\n",
    "p_max_bins = 25                # Maximum number of parallel bins allowed when calculating dynamically. (Reduced from 50 to 15)\n",
    "\n",
    "# Filtering\n",
    "workspaces_df = spark.sql(f\"Select DISTINCT A.Workspace_Id from SemanticModels A INNER JOIN Workspaces B ON   B.Id = A.Workspace_Id INNER JOIN Capacities C ON  B.Capacity_Id = C.Capacity_Id  WHERE C.Capacity_Id  IN ('CapacityIDs')\") \n",
    "p_only_workspaces = [row.Workspace_Id for row in workspaces_df.collect()]\n",
    "#p_only_workspaces = ['Workspace1','Workspace2','Workspace3']          # List of workspace names or IDs to *include*. Leave empty to process all allowed workspaces.\n",
    "#p_only_workspaces = []\n",
    "# Example: ['Migration_Clone_1','Migration_Clone_2','f1a1b1c1-d1e1-f1a1-b1c1-d1e1f1a1b1c1']\n",
    "p_exclude_workspaces = []                                 # List of workspace names or IDs to *exclude*. Only used if p_only_workspaces is empty.\n",
    "# Example: ['Default Workspace','My Other Workspace']\n",
    "\n",
    "p_min_model_size_gb = 10       # Optional: Set minimum model size in GB to process fully. Models smaller than this will be marked 'Ignored (Size)'. Set to 0 or None to disable.\n",
    "\n",
    "# Backoff Strategy (for API throttling)\n",
    "p_backoff_threshold = 30        # Seconds: If a single model size check takes longer than this, apply backoff delay.\n",
    "p_backoff_delay = 90            # Seconds: Initial delay to apply when backoff is triggered.\n",
    "p_backoff_delay_multiple = 2    # Multiplier: Increase delay by this factor on subsequent backoffs.\n",
    "\n",
    "# Internal Parameter (Do Not Change Manually)\n",
    "_p_sub_bin = 0                  # Used internally by runMultiple to identify the sub-process bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c876c-af3b-4759-a170-a611d1fb9833",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import notebookutils\n",
    "import pandas as pd\n",
    "import datetime, time, math, json, requests\n",
    "import numpy as np\n",
    "from uuid import UUID\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n",
    "import sempy_labs as labs\n",
    "import sempy_labs.admin as labs_admin\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType\n",
    "\n",
    "# --- Icon Definitions ---\n",
    "icons = {\n",
    "    \"start\": \"🚀\", \"finish\": \"🏁\", \"main\": \" orchestrator \", \"sub\": \" sub-process \",\n",
    "    \"step\": \"⚙️\", \"info\": \"ℹ️\", \"success\": \"✅\", \"warning\": \"⚠️\", \"error\": \"❌\",\n",
    "    \"data\": \"💾\", \"spark\": \"✨\", \"parallel\": \"🔀\", \"report\": \"📊\", \"key\": \"🔑\"\n",
    "}\n",
    "\n",
    "# --- Initialization ---\n",
    "spark = SparkSession.builder.appName(\"LargeModelDiscovery\").getOrCreate()\n",
    "print(f\"{icons['start']} [{datetime.datetime.now()}] CMF - Large Semantic Model Discovery Notebook Initialized {icons['start']}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "#------------------------------------------------------------------------------------#\n",
    "#                                HELPER FUNCTIONS                                    #\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "def _is_valid_uuid(guid_string: str) -> bool:\n",
    "    try:\n",
    "        UUID(guid_string, version=4); return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def _resolve_workspace_ids(p_list: list, all_ws_df: pd.DataFrame, list_name: str) -> list:\n",
    "    \"\"\"Converts a list of workspace names or IDs into a list of IDs only.\"\"\"\n",
    "    if not p_list: return []\n",
    "    print(f\"{icons['info']} Resolving workspace names to IDs for '{list_name}'...\")\n",
    "    resolved_ids = []\n",
    "    \n",
    "    # Prepare a lookup series for faster, case-insensitive matching\n",
    "    name_to_id_map = all_ws_df.set_index(all_ws_df['Name'].str.lower())['Id']\n",
    "\n",
    "    for item in p_list:\n",
    "        if _is_valid_uuid(item):\n",
    "            resolved_ids.append(item)\n",
    "            print(f\"  {icons['success']} Found valid ID: '{item}'\")\n",
    "        else:\n",
    "            item_lower = item.lower()\n",
    "            if item_lower in name_to_id_map:\n",
    "                resolved_id = name_to_id_map[item_lower]\n",
    "                resolved_ids.append(resolved_id)\n",
    "                print(f\"  {icons['success']} Resolved '{item}' to ID: {resolved_id}\")\n",
    "            else:\n",
    "                print(f\"  {icons['warning']} Workspace name '{item}' not found. It will be skipped.\")\n",
    "    return resolved_ids\n",
    "\n",
    "def _grant_workspace_access(workspaces: list, all_ws_df: pd.DataFrame, user_principal: str) -> list:\n",
    "    \"\"\"Grants 'Member' access, logs it, and returns a list of modified workspaces.\"\"\"\n",
    "    if not workspaces:\n",
    "        print(f\"{icons['info']} 'p_only_workspaces' is empty. Skipping permission grants.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\\\n{icons['key']} STEP: Granting current user '{user_principal}' access to specified workspaces...\")\n",
    "    modified_ws = []\n",
    "    \n",
    "    resolved_ids = _resolve_workspace_ids(workspaces, all_ws_df, \"p_only_workspaces\")\n",
    "\n",
    "    for ws_id in resolved_ids:\n",
    "        print(f\"  Processing workspace: {ws_id}\")\n",
    "        try:\n",
    "            existing_users_df = labs_admin.list_workspace_users(workspace=ws_id)\n",
    "            user_exists = not existing_users_df[existing_users_df['Identifier'].str.lower() == user_principal.lower()].empty\n",
    "            \n",
    "            if user_exists:\n",
    "                print(f\"    {icons['success']} User already has access. No action needed.\")\n",
    "            else:\n",
    "                print(f\"    {icons['info']} User not found. Granting 'Member' access...\")\n",
    "                labs_admin.add_user_to_workspace(workspace=ws_id, user=user_principal, role=\"Member\")\n",
    "                modified_ws.append(ws_id)\n",
    "                print(f\"    {icons['success']} Successfully granted 'Member' role.\")\n",
    "                \n",
    "                # --- NEW: Log the permission grant to the Delta table ---\n",
    "                try:\n",
    "                    log_sql = f\"\"\"\n",
    "                        INSERT INTO permission_audit_log (Workspace_Id, User_Principal, Role_Granted, Grant_Timestamp, Status)\n",
    "                        VALUES ('{ws_id}', '{user_principal}', 'Member', current_timestamp(), 'Granted')\n",
    "                    \"\"\"\n",
    "                    spark.sql(log_sql)\n",
    "                    print(f\"    {icons['data']} Successfully logged permission grant.\")\n",
    "                except Exception as log_e:\n",
    "                    print(f\"    {icons['error']} CRITICAL: Failed to log permission grant for workspace '{ws_id}'. Manual cleanup may be required. Error: {log_e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    {icons['error']} Could not check or grant permissions for workspace '{ws_id}'. Error: {e}\")\n",
    "            \n",
    "    return modified_ws\n",
    "\n",
    "def _revoke_workspace_access(modified_workspaces: list, user_principal: str):\n",
    "    \"\"\"Revokes 'Member' access and updates the audit log.\"\"\"\n",
    "    if not modified_workspaces:\n",
    "        print(f\"\\\\n{icons['key']} CLEANUP: No workspace permissions to revoke for this run.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\\\n{icons['key']} CLEANUP: Revoking 'Member' access for '{user_principal}' from {len(modified_workspaces)} workspace(s)...\")\n",
    "    for ws_id in modified_workspaces:\n",
    "        try:\n",
    "            print(f\"  Removing user from workspace: '{ws_id}'\")\n",
    "            labs_admin.delete_user_from_workspace(workspace=ws_id, user=user_principal)\n",
    "            print(f\"    {icons['success']} Successfully revoked permissions via API.\")\n",
    "            \n",
    "            # --- NEW: Update the audit log ---\n",
    "            try:\n",
    "                update_sql = f\"\"\"\n",
    "                    UPDATE permission_audit_log\n",
    "                    SET Status = 'Revoked', Revoke_Timestamp = current_timestamp()\n",
    "                    WHERE Workspace_Id = '{ws_id}' AND User_Principal = '{user_principal}' AND Status = 'Granted'\n",
    "                \"\"\"\n",
    "                spark.sql(update_sql)\n",
    "                print(f\"    {icons['data']} Successfully updated audit log to 'Revoked'.\")\n",
    "            except Exception as log_e:\n",
    "                print(f\"    {icons['warning']} Could not update audit log for workspace '{ws_id}'. The permission was revoked, but the log may be out of sync. Error: {log_e}\")\n",
    "\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if http_err.response.status_code == 404:\n",
    "                print(f\"    {icons['success']} User was not found (already removed). Cleanup successful.\")\n",
    "            else:\n",
    "                print(f\"    {icons['warning']} Could not remove user. Status: {http_err.response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {icons['error']} An unexpected error occurred during revocation. Manual check may be needed. Error: {e}\")\n",
    "\n",
    "def _initialize_permission_audit_table():\n",
    "    \"\"\"Creates the permission_audit_log Delta table if it does not exist.\"\"\"\n",
    "    table_name = \"permission_audit_log\"\n",
    "    print(f\"{icons['step']} Initializing or validating schema for Delta table: '{table_name}'\")\n",
    "    \n",
    "    # Define the schema for tracking permission changes\n",
    "    schema = StructType([\n",
    "        StructField(\"Workspace_Id\", StringType(), False),\n",
    "        StructField(\"User_Principal\", StringType(), False),\n",
    "        StructField(\"Role_Granted\", StringType(), True),\n",
    "        StructField(\"Grant_Timestamp\", TimestampType(), True),\n",
    "        StructField(\"Revoke_Timestamp\", TimestampType(), True),\n",
    "        StructField(\"Status\", StringType(), True)  # e.g., 'Granted', 'Revoked'\n",
    "    ])\n",
    "    \n",
    "    # Create an empty DataFrame with the schema\n",
    "    empty_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # Create the table if it doesn't exist, preserving the schema\n",
    "    empty_df.write.format(\"delta\").mode(\"ignore\").saveAsTable(table_name)\n",
    "    print(f\"  {icons['success']} Table '{table_name}' is ready.\")\n",
    "\n",
    "#------------------------------------------------------------------------------------#\n",
    "#                            ORCHESTRATOR (MAIN) LOGIC                               #\n",
    "#------------------------------------------------------------------------------------#\n",
    "def run_orchestrator():\n",
    "    \"\"\"Main function for the orchestrator notebook instance.\"\"\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(f\"{icons['main']} Running in Orchestrator Mode (_p_sub_bin = 0) at {start_time} {icons['main']}\")\n",
    "    \n",
    "    workspaces_modified = []\n",
    "    current_user = \"\"\n",
    "    \n",
    "    try:\n",
    "        # --- NEW: Step -1: Initialize Audit Table ---\n",
    "        _initialize_permission_audit_table()\n",
    "\n",
    "        current_user = mssparkutils.env.getUserName()\n",
    "        print(f\"Current User Principal Name (UPN): {current_user}\")\n",
    "\n",
    "        # --- NEW: Step -0.5: Pre-run Cleanup of Orphaned Permissions ---\n",
    "        print(f\"\\\\n{icons['key']} STEP: Checking for orphaned permissions from previous runs...\")\n",
    "        try:\n",
    "            orphaned_df = spark.sql(f\"SELECT Workspace_Id FROM permission_audit_log WHERE Status = 'Granted' AND User_Principal = '{current_user}'\")\n",
    "            orphaned_workspaces = [row.Workspace_Id for row in orphaned_df.collect()]\n",
    "            \n",
    "            if orphaned_workspaces:\n",
    "                print(f\"  {icons['warning']} Found {len(orphaned_workspaces)} orphaned permission(s) for '{current_user}'. Attempting to revoke now.\")\n",
    "                _revoke_workspace_access(orphaned_workspaces, current_user)\n",
    "            else:\n",
    "                print(f\"  {icons['success']} No orphaned permissions found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {icons['error']} Could not check for or clean up orphaned permissions. Error: {e}\")\n",
    "            \n",
    "        # --- Step 0: Permissions ---\n",
    "        current_user = mssparkutils.env.getUserName()\n",
    "        print(f\"Current User Principal Name (UPN): {current_user}\")\n",
    "        \n",
    "        print(f\"\\n{icons['step']} STEP 0: Permissions & Workspace Resolution\")\n",
    "        all_ws_df = labs_admin.list_workspaces()\n",
    "        print(f\"  {icons['info']} Fetched {len(all_ws_df)} workspaces from the tenant.\")\n",
    "        workspaces_modified = _grant_workspace_access(p_only_workspaces, all_ws_df, current_user)\n",
    "\n",
    "        # --- Step 1: List & Filter Models ---\n",
    "        print(f\"\\n{icons['step']} STEP 1: Discovering Large Format Semantic Models\")\n",
    "        df_models_all = labs_admin.list_datasets()\n",
    "        df_large_models = df_models_all[df_models_all['Target Storage Mode'] == 'PremiumFiles'].copy()\n",
    "        print(f\"  {icons['info']} Found {len(df_large_models)} total Large Format models in the tenant.\")\n",
    "\n",
    "        resolved_include_list = _resolve_workspace_ids(p_only_workspaces, all_ws_df, \"p_only_workspaces\")\n",
    "        resolved_exclude_list = _resolve_workspace_ids(p_exclude_workspaces, all_ws_df, \"p_exclude_workspaces\")\n",
    "\n",
    "        if resolved_include_list:\n",
    "            df_models_filtered = df_large_models[df_large_models[\"Workspace Id\"].isin(resolved_include_list)]\n",
    "            print(f\"  {icons['success']} Applied 'include' filter. Models remaining: {len(df_models_filtered)}\")\n",
    "        elif resolved_exclude_list:\n",
    "            df_models_filtered = df_large_models[~df_large_models[\"Workspace Id\"].isin(resolved_exclude_list)]\n",
    "            print(f\"  {icons['success']} Applied 'exclude' filter. Models remaining: {len(df_models_filtered)}\")\n",
    "        else:\n",
    "            df_models_filtered = df_large_models\n",
    "            print(f\"  {icons['warning']} No workspace filters applied. Processing all found models.\")\n",
    "        \n",
    "        num_models = len(df_models_filtered)\n",
    "        if num_models == 0:\n",
    "            print(f\"\\n{icons['finish']} No models match the criteria. Halting execution.\")\n",
    "            return\n",
    "\n",
    "        # --- Step 2: Binning ---\n",
    "        print(f\"\\n{icons['step']} STEP 2: Calculating Bins for Parallel Processing\")\n",
    "        if p_num_bins > 0:\n",
    "            num_bins = p_num_bins\n",
    "        elif p_target_models_per_bin > 0:\n",
    "            num_bins = min(math.ceil(num_models / p_target_models_per_bin), p_max_bins, num_models)\n",
    "        else:\n",
    "            num_bins = 1\n",
    "        num_bins = max(1, num_bins)\n",
    "        print(f\"  {icons['info']} Strategy: {num_bins} bin(s) will be used to process {num_models} models.\")\n",
    "\n",
    "        df_models_filtered = df_models_filtered.reset_index(drop=True)\n",
    "        df_models_filtered['Bin'] = (np.floor(df_models_filtered.index * num_bins / num_models) + 1).astype(int)\n",
    "\n",
    "        # --- Step 3: Prepare and Save to Delta ---\n",
    "        print(f\"\\n{icons['step']} STEP 3: Preparing and Saving Task List to Delta Table\")\n",
    "        delta_table_name = \"models_discovery_status\"\n",
    "        df_to_save = df_models_filtered[['Dataset Id', 'Dataset Name', 'Workspace Id', 'Bin']].rename(\n",
    "            columns={\"Dataset Id\": \"Dataset_Id\", \"Dataset Name\": \"Dataset_Name\", \"Workspace Id\": \"Workspace_Id\"}\n",
    "        )\n",
    "        spark_df = spark.createDataFrame(df_to_save)\n",
    "        spark_df = (spark_df\n",
    "                    .withColumn(\"Status\", lit(\"Pending\"))\n",
    "                    .withColumn(\"Size\", lit(-1).cast(LongType()))\n",
    "                    .withColumn(\"Message\", lit(\"\"))\n",
    "                    .withColumn(\"Last_Updated\", current_timestamp()))\n",
    "\n",
    "        spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"Bin\").saveAsTable(delta_table_name)\n",
    "        print(f\"  {icons['data']} Successfully saved {spark_df.count()} tasks to Delta table '{delta_table_name}'.\")\n",
    "\n",
    "        # --- Step 4: Execute Processing ---\n",
    "        print(f\"\\n{icons['step']} STEP 4: Executing Model Size Discovery\")\n",
    "        bin_rows = spark.sql(f\"SELECT DISTINCT Bin FROM {delta_table_name} ORDER BY Bin\").collect()\n",
    "\n",
    "        if len(bin_rows) > 1:\n",
    "            print(f\"  {icons['parallel']} Launching {len(bin_rows)} parallel sub-processes...\")\n",
    "            activities = []\n",
    "            for row in bin_rows:\n",
    "                bin_num = row['Bin']\n",
    "                activities.append({\n",
    "                    \"name\": f\"Discovery_Sub_Bin_{bin_num}\", \"path\": notebookutils.runtime.context['currentNotebookName'],\n",
    "                    \"timeoutPerCellInSeconds\": 10800,\n",
    "                    \"args\": {\"_p_sub_bin\": bin_num, \"p_min_model_size_gb\": p_min_model_size_gb, \n",
    "                             \"p_backoff_threshold\": p_backoff_threshold, \"p_backoff_delay\": p_backoff_delay,\n",
    "                             \"p_backoff_delay_multiple\": p_backoff_delay_multiple},\n",
    "                    \"retry\": 0\n",
    "                })\n",
    "            exit_values = notebookutils.notebook.runMultiple({\"activities\": activities})\n",
    "            print(f\"\\n{icons['success']} Parallel execution finished.\")\n",
    "            print(\"  Sub-process exit summaries:\")\n",
    "            for val in exit_values: print(f\"    - {val}: {exit_values[val].get('exitVal', 'No exit value')}\")\n",
    "        elif len(bin_rows) == 1:\n",
    "            print(f\"  {icons['info']} Only one bin found. Running sequentially in this notebook...\")\n",
    "            global _p_sub_bin\n",
    "            _p_sub_bin = bin_rows[0]['Bin']\n",
    "            run_subprocess() # Run directly\n",
    "        else:\n",
    "            print(f\"  {icons['warning']} No bins with models found. Skipping processing.\")\n",
    "\n",
    "        # --- Step 5: Final Report ---\n",
    "        print(f\"\\n{icons['report']} STEP 5: Generating Final Summary Report {icons['report']}\")\n",
    "        summary_df = spark.sql(f\"SELECT Status, COUNT(*) as Count, SUM(CASE WHEN Size >= 0 THEN Size ELSE 0 END) as TotalSizeBytes FROM {delta_table_name} GROUP BY Status\")\n",
    "        print(\"\\n--- Final Status Counts ---\")\n",
    "        summary_df.show()\n",
    "\n",
    "        failed_models_df = spark.sql(f\"SELECT Dataset_Name, Workspace_Id, Message FROM {delta_table_name} WHERE Status = 'Failure'\")\n",
    "        if not failed_models_df.isEmpty():\n",
    "            print(\"\\n--- Models with Failures ---\")\n",
    "            failed_models_df.show(truncate=False)\n",
    "\n",
    "        min_size_bytes = (p_min_model_size_gb or 0) * (1024**3)\n",
    "        success_models_df = spark.sql(f\"\"\"\n",
    "            SELECT Dataset_Name, Workspace_Id, ROUND(Size / (1024*1024*1024), 3) as Size_GB \n",
    "            FROM {delta_table_name} \n",
    "            WHERE Status = 'Success' AND Size >= {min_size_bytes}\n",
    "            ORDER BY Size DESC\n",
    "        \"\"\")\n",
    "        if not success_models_df.isEmpty():\n",
    "            print(f\"\\n--- Successfully Processed Models (>= {p_min_model_size_gb or 0} GB) ---\")\n",
    "            success_models_df.show(truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{icons['error']} A critical error occurred in the orchestrator: {e}\")\n",
    "    finally:\n",
    "        # --- Step 6: Cleanup ---(This now only cleans up permissions from the CURRENT run)\n",
    "        _revoke_workspace_access(workspaces_modified, current_user)\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{icons['finish']} Orchestrator Finished at {end_time}. Total Time: {end_time - start_time} {icons['finish']}\")\n",
    "        notebookutils.notebook.exit(\"Orchestration complete.\")\n",
    "\n",
    "#------------------------------------------------------------------------------------#\n",
    "#                              SUB-PROCESS LOGIC                                     #\n",
    "#------------------------------------------------------------------------------------#\n",
    "def run_subprocess():\n",
    "    \"\"\"Main function for the sub-process notebook instance.\"\"\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(f\"\\n{icons['sub']} Running in Sub-Process Mode for Bin #{_p_sub_bin} at {start_time} {icons['sub']}\")\n",
    "    \n",
    "    delta_table_name = \"models_discovery_status\"\n",
    "    success, error, ignored_default, ignored_size, ignored_name = 0, 0, 0, 0, 0\n",
    "    \n",
    "    try:\n",
    "        models_to_process = spark.sql(f\"SELECT Dataset_Id, Dataset_Name, Workspace_Id FROM {delta_table_name} WHERE Bin = {_p_sub_bin} AND Status = 'Pending'\").collect()\n",
    "    except Exception as e:\n",
    "        notebookutils.notebook.exit(f\"❌ FATAL: Could not query models for Bin #{_p_sub_bin}. Error: {e}\")\n",
    "\n",
    "    total_models = len(models_to_process)\n",
    "    print(f\"  {icons['info']} Found {total_models} models to process in this bin.\")\n",
    "    min_size_bytes = (p_min_model_size_gb or 0) * (1024**3)\n",
    "\n",
    "    for i, row in enumerate(models_to_process):\n",
    "        print(f\"\\n  ({i+1}/{total_models}) 🔍 Processing model: '{row['Dataset_Name']}'\")\n",
    "        status, size, message = \"Failure\", -1, \"\"\n",
    "        \n",
    "        try:\n",
    "            if row['Dataset_Name'] == 'Report Usage Metrics Model':\n",
    "                status, size, message = \"Ignored (Name Filter)\", 0, \"Model excluded by name.\"\n",
    "                ignored_name += 1\n",
    "            elif labs.is_default_semantic_model(dataset=row['Dataset_Name'], workspace=row['Workspace_Id']):\n",
    "                status, size, message = \"Ignored (Default)\", 0, \"Default semantic model.\"\n",
    "                ignored_default += 1\n",
    "            else:\n",
    "                retrieved_size = labs.get_semantic_model_size(dataset=row['Dataset_Name'], workspace=row['Workspace_Id'])\n",
    "                if min_size_bytes > 0 and retrieved_size < min_size_bytes:\n",
    "                    status, size, message = \"Ignored (Size)\", retrieved_size, f\"Model size is smaller than {p_min_model_size_gb} GB.\"\n",
    "                    ignored_size += 1\n",
    "                else:\n",
    "                    status, size, message = \"Success\", retrieved_size, f\"Size retrieved successfully: {round(retrieved_size/(1024**3), 3)} GB.\"\n",
    "                    success += 1\n",
    "        except Exception as e:\n",
    "            message = f\"Error getting size: {str(e)}\"\n",
    "            error += 1\n",
    "            print(f\"    {icons['error']} {message}\")\n",
    "\n",
    "        print(f\"    -> Status: {status}\")\n",
    "        update_sql = f\"\"\"\n",
    "            UPDATE {delta_table_name}\n",
    "            SET Status = '{status}', Size = {size}, Message = '{str(message).replace(\"'\", \"''\")[:3900]}', Last_Updated = current_timestamp()\n",
    "            WHERE Dataset_Id = '{row['Dataset_Id']}' AND Bin = {_p_sub_bin}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            spark.sql(update_sql)\n",
    "        except Exception as update_e:\n",
    "            print(f\"    {icons['error']} Failed to update Delta table for this model. Error: {update_e}\")\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    summary = (f\"Bin #{_p_sub_bin} Finished. Processed: {total_models}. ✅Success: {success}, ❌Errors: {error}, \"\n",
    "               f\"Ignored: {ignored_default+ignored_size+ignored_name}. Duration: {end_time - start_time}\")\n",
    "    print(f\"\\n{icons['finish']} {summary}\")\n",
    "    notebookutils.notebook.exit(summary)\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    if _p_sub_bin == 0:\n",
    "        run_orchestrator()\n",
    "    else:\n",
    "        run_subprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167e2a5-f3e2-4bd6-ac88-cd2099b83af9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Final Cell (Optional - Display Results if run interactively after completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb9494-498b-448a-a1a2-f8845ccfdc89",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-30T14:41:31.263786Z",
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "cancelled",
       "parent_msg_id": "cf7e7d2f-1f06-4cf5-a944-33750001c96f",
       "queued_time": "2025-07-30T14:23:52.3977817Z",
       "session_id": "a417a8a5-dba1-4a02-9e81-ca1019c93274",
       "session_start_time": null,
       "spark_pool": null,
       "state": "cancelled",
       "statement_id": -1,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, a417a8a5-dba1-4a02-9e81-ca1019c93274, -1, Cancelled, , Cancelled)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Final Cell (Optional - Display Results if run interactively after completion) ---\n",
    "# This cell will only execute if the notebook runs to completion without an explicit exit,\n",
    "# or if you run it manually after the main process finishes.\n",
    "# It's generally better to rely on the summary generated in Step 9 of the main block.\n",
    "delta_table_name = \"models_discovery_status\"\n",
    "# print(\"\\n --- Displaying Sample of Final Results ---\")\n",
    "try:\n",
    "     final_results_df = spark.sql(f\"SELECT * FROM {delta_table_name} ORDER BY Bin, `Dataset_Name` LIMIT 1000\")\n",
    "     display(final_results_df)\n",
    "except Exception as e:\n",
    "     print(f\"Could not display final results from {delta_table_name}. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "environment": {
    "environmentId": "781fa191-5dfa-4a9f-88c6-064450198158",
    "workspaceId": "b1317605-ef25-462c-9dda-82a68b53f242"
   },
   "lakehouse": {
    "default_lakehouse": "dd9a80b3-9558-405a-b16e-2e953ac61208",
    "default_lakehouse_name": "metadatascan_global_lh_0725",
    "default_lakehouse_workspace_id": "b1317605-ef25-462c-9dda-82a68b53f242",
    "known_lakehouses": [
     {
      "id": "dd9a80b3-9558-405a-b16e-2e953ac61208"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
